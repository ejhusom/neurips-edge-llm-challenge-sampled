# HumanEval: Hand-Written Evaluation Set

- Description: A dataset of 164 original programming problems with unit tests[^1]. 
- Link to source: [https://github.com/openai/human-eval](https://github.com/openai/human-eval)
- No. of tasks for the full dataset: 164
- No. of tasks for the reduced dataset: - 

No sampling was performed on this dataset since it contains fewer than 200 tasks in total (164 tasks). The entire dataset was included as-is without modification or reduction. The original dataset structure has been preserved.

[^1]: Mark Chen, et al. 2021. [Evaluating large language models trained on code.](https://arxiv.org/pdf/2107.03374) arXiv preprint arXiv:2107.03374 (2021).
